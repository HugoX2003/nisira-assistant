"""
PostgreSQL Vector Store
=======================

Almacenamiento de embeddings directamente en PostgreSQL
usando la extensi√≥n pgvector para b√∫squeda eficiente.
"""

import os
import logging
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import uuid
import numpy as np

try:
    import psycopg2
    from psycopg2.extras import Json, execute_values
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False

logger = logging.getLogger(__name__)


class PostgresVectorStore:
    """
    Gestor de embeddings usando PostgreSQL + pgvector
    """
    
    def __init__(self, database_url: str = None):
        self.database_url = database_url or os.getenv('DATABASE_URL')
        self.conn = None
        self.embedding_dim = 768  # Dimensi√≥n de embeddings de Google
        self.use_vector_type = False  # Rastrear si pgvector est√° disponible
        
        if not PSYCOPG2_AVAILABLE:
            logger.error("‚ùå psycopg2 no disponible")
            return
        
        self._initialize_connection()
        self._ensure_table_exists()
    
    def _initialize_connection(self):
        """Conectar a PostgreSQL"""
        try:
            self.conn = psycopg2.connect(self.database_url)
            self.conn.autocommit = False
            logger.info("‚úÖ Conectado a PostgreSQL para embeddings")
        except Exception as e:
            logger.error(f"‚ùå Error conectando a PostgreSQL: {e}")
            self.conn = None
    
    def _ensure_table_exists(self):
        """Crear tabla de embeddings si no existe"""
        if not self.conn:
            return
        
        try:
            with self.conn.cursor() as cur:
                # Habilitar extensi√≥n pgvector (si est√° disponible)
                use_vector = False
                try:
                    cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
                    self.conn.commit()
                    use_vector = True
                    self.use_vector_type = True
                    logger.info("‚úÖ Extensi√≥n pgvector habilitada")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è pgvector no disponible, usando JSONB para vectores: {e}")
                    self.conn.rollback()
                    use_vector = False
                    self.use_vector_type = False
                
                # Verificar si la tabla existe y su tipo de columna
                cur.execute("""
                    SELECT column_name, data_type, udt_name
                    FROM information_schema.columns
                    WHERE table_name = 'rag_embeddings' 
                    AND column_name = 'embedding_vector'
                """)
                existing_column = cur.fetchone()
                
                # Si la tabla existe pero con tipo incorrecto, migrar
                if existing_column:
                    column_type = existing_column[2]  # udt_name
                    
                    if use_vector and column_type != 'vector':
                        logger.warning(f"‚ö†Ô∏è Columna embedding_vector es tipo '{column_type}', migrando a vector...")
                        
                        # Renombrar tabla antigua
                        cur.execute("ALTER TABLE IF EXISTS rag_embeddings RENAME TO rag_embeddings_old;")
                        self.conn.commit()
                        logger.info("‚úÖ Tabla antigua respaldada como rag_embeddings_old")
                
                # Crear tabla
                if use_vector:
                    # Con pgvector
                    cur.execute(f"""
                        CREATE TABLE IF NOT EXISTS rag_embeddings (
                            id UUID PRIMARY KEY,
                            chunk_text TEXT NOT NULL,
                            embedding_vector vector({self.embedding_dim}),
                            metadata JSONB,
                            created_at TIMESTAMP DEFAULT NOW(),
                            updated_at TIMESTAMP DEFAULT NOW()
                        );
                    """)
                    self.conn.commit()
                    logger.info("‚úÖ Tabla rag_embeddings creada con tipo vector")
                    
                    # Migrar datos de tabla antigua si existe
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_name = 'rag_embeddings_old'
                        )
                    """)
                    if cur.fetchone()[0]:
                        logger.info("üîÑ Migrando datos de rag_embeddings_old...")
                        try:
                            # Migrar datos - convertir TEXT/JSONB a vector
                            cur.execute(f"""
                                INSERT INTO rag_embeddings (id, chunk_text, embedding_vector, metadata, created_at, updated_at)
                                SELECT 
                                    id,
                                    chunk_text,
                                    CASE 
                                        WHEN embedding_vector::text LIKE '[%' THEN embedding_vector::text::vector({self.embedding_dim})
                                        ELSE NULL
                                    END as embedding_vector,
                                    metadata,
                                    created_at,
                                    updated_at
                                FROM rag_embeddings_old
                                WHERE embedding_vector IS NOT NULL
                            """)
                            migrated = cur.rowcount
                            self.conn.commit()
                            logger.info(f"‚úÖ {migrated} embeddings migrados exitosamente")
                            
                            # Eliminar tabla antigua
                            cur.execute("DROP TABLE rag_embeddings_old;")
                            self.conn.commit()
                            logger.info("‚úÖ Tabla antigua eliminada")
                        except Exception as migrate_error:
                            logger.error(f"‚ùå Error migrando datos: {migrate_error}")
                            self.conn.rollback()
                    
                    # Crear √≠ndice IVFFlat (solo si hay suficientes datos)
                    try:
                        # Verificar si ya existe el √≠ndice
                        cur.execute("""
                            SELECT COUNT(*) FROM pg_indexes 
                            WHERE tablename = 'rag_embeddings' 
                            AND indexname = 'idx_embedding_vector'
                        """)
                        if cur.fetchone()[0] == 0:
                            # Crear √≠ndice solo si hay m√°s de 1000 filas (recomendaci√≥n pgvector)
                            cur.execute("SELECT COUNT(*) FROM rag_embeddings")
                            row_count = cur.fetchone()[0]
                            
                            if row_count > 1000:
                                logger.info(f"üìä Creando √≠ndice IVFFlat para {row_count} embeddings...")
                                cur.execute(f"""
                                    CREATE INDEX idx_embedding_vector 
                                    ON rag_embeddings 
                                    USING ivfflat (embedding_vector vector_cosine_ops)
                                    WITH (lists = 100);
                                """)
                                self.conn.commit()
                                logger.info("‚úÖ √çndice IVFFlat creado")
                            else:
                                logger.info(f"‚ÑπÔ∏è  Solo {row_count} embeddings, √≠ndice IVFFlat se crear√° despu√©s de 1000+")
                    except Exception as idx_error:
                        logger.warning(f"‚ö†Ô∏è No se pudo crear √≠ndice IVFFlat: {idx_error}")
                        self.conn.rollback()
                else:
                    # Sin pgvector, usar JSONB
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS rag_embeddings (
                            id UUID PRIMARY KEY,
                            chunk_text TEXT NOT NULL,
                            embedding_vector JSONB,
                            metadata JSONB,
                            created_at TIMESTAMP DEFAULT NOW(),
                            updated_at TIMESTAMP DEFAULT NOW()
                        );
                    """)
                    self.conn.commit()
                    logger.info("‚úÖ Tabla rag_embeddings creada con tipo JSONB")
                    
                    # √çndice para metadata
                    try:
                        cur.execute("""
                            CREATE INDEX IF NOT EXISTS idx_metadata 
                            ON rag_embeddings USING gin(metadata);
                        """)
                        self.conn.commit()
                        logger.info("‚úÖ √çndice GIN para metadata creado")
                    except Exception as idx_error:
                        logger.warning(f"‚ö†Ô∏è Error creando √≠ndice metadata: {idx_error}")
                        self.conn.rollback()
                
                logger.info("‚úÖ Tabla rag_embeddings lista")
                
        except Exception as e:
            logger.error(f"‚ùå Error creando tabla: {e}")
            if self.conn:
                self.conn.rollback()
    
    def is_ready(self) -> bool:
        """Verificar si el store est√° listo"""
        return PSYCOPG2_AVAILABLE and self.conn is not None
    
    def add_documents(self, documents: List[Dict[str, Any]], 
                     embeddings: List[List[float]]) -> bool:
        """
        Agregar documentos con embeddings a PostgreSQL
        
        Args:
            documents: Lista de documentos con metadatos
            embeddings: Lista de embeddings correspondientes
        
        Returns:
            True si fue exitoso
        """
        logger.info(f"üîµ PostgreSQL add_documents llamado con {len(documents)} docs, {len(embeddings)} embeddings")
        logger.info(f"üîµ DB connection status: {self.conn is not None}")
        logger.info(f"üîµ DB URL configured: {bool(self.database_url)}")
        
        if not self.is_ready():
            logger.error("‚ùå PostgreSQL no est√° listo")
            logger.error(f"   - PSYCOPG2_AVAILABLE: {PSYCOPG2_AVAILABLE}")
            logger.error(f"   - self.conn: {self.conn}")
            return False
        
        if len(documents) != len(embeddings):
            logger.error("‚ùå N√∫mero de documentos y embeddings no coinciden")
            return False
        
        try:
            with self.conn.cursor() as cur:
                # Preparar datos para inserci√≥n batch
                now = datetime.now()
                values = []
                for doc, embedding in zip(documents, embeddings):
                    if embedding is None:
                        continue
                    
                    doc_id = str(uuid.uuid4())
                    chunk_text = doc['text']
                    metadata = doc.get('metadata', {})
                    metadata['doc_id'] = doc_id
                    metadata['added_at'] = now.isoformat()
                    
                    if self.use_vector_type:
                        # Usar formato de array para pgvector: [0.1, 0.2, ...]
                        embedding_str = '[' + ','.join(str(x) for x in embedding) + ']'
                    else:
                        # Convertir embedding a formato JSON para JSONB
                        embedding_str = json.dumps(embedding)
                    
                    values.append((
                        doc_id,
                        chunk_text,
                        embedding_str,
                        Json(metadata),
                        now,
                        now
                    ))
                
                if not values:
                    logger.warning("‚ö†Ô∏è No hay documentos v√°lidos para insertar")
                    return True
                
                logger.info(f"üîµ Preparados {len(values)} valores para insertar en PostgreSQL")
                logger.info(f"üîµ Usando tipo: {'vector' if self.use_vector_type else 'JSONB'}")
                logger.info(f"üîµ Muestra del primer valor: id={values[0][0][:8]}..., texto_len={len(values[0][1])}")
                
                # Inserci√≥n batch con el tipo correcto
                logger.info("üîµ Ejecutando INSERT batch en PostgreSQL...")
                if self.use_vector_type:
                    # Con pgvector
                    execute_values(
                        cur,
                        """
                        INSERT INTO rag_embeddings 
                        (id, chunk_text, embedding_vector, metadata, created_at, updated_at)
                        VALUES %s
                        """,
                        values,
                        template="(%s, %s, %s::vector, %s, %s, %s)"
                    )
                else:
                    # Con JSONB
                    execute_values(
                        cur,
                        """
                        INSERT INTO rag_embeddings 
                        (id, chunk_text, embedding_vector, metadata, created_at, updated_at)
                        VALUES %s
                        """,
                        values,
                        template="(%s, %s, %s::jsonb, %s, %s, %s)"
                    )
                
                logger.info("üîµ INSERT completado, ejecutando COMMIT...")
                self.conn.commit()
                logger.info(f"‚úÖ {len(values)} documentos CONFIRMADOS en PostgreSQL (COMMIT exitoso)")
                
                # Verificar que realmente se insertaron
                cur.execute("SELECT COUNT(*) FROM rag_embeddings")
                total_count = cur.fetchone()[0]
                logger.info(f"üìä Total de documentos en tabla rag_embeddings: {total_count}")
                
                return True
                
        except Exception as e:
            logger.error(f"‚ùå Error insertando documentos: {e}")
            if self.conn:
                self.conn.rollback()
            return False
    
    def search_similar(self, query_embedding: List[float], 
                      n_results: int = 5,
                      similarity_threshold: float = 0.15) -> List[Dict[str, Any]]:
        """
        Buscar documentos similares usando embedding de consulta
        
        Args:
            query_embedding: Embedding de la consulta
            n_results: N√∫mero de resultados
            similarity_threshold: Umbral m√≠nimo de similitud
        
        Returns:
            Lista de documentos similares con scores
        """
        if not self.is_ready():
            logger.error("‚ùå PostgreSQL no est√° listo")
            return []
        
        try:
            with self.conn.cursor() as cur:
                if self.use_vector_type:
                    # B√∫squeda con pgvector (mucho m√°s eficiente)
                    query_vector_str = '[' + ','.join(str(x) for x in query_embedding) + ']'
                    
                    cur.execute("""
                        SELECT 
                            id,
                            chunk_text,
                            metadata,
                            1 - (embedding_vector <=> %s::vector) as similarity
                        FROM rag_embeddings
                        ORDER BY embedding_vector <=> %s::vector
                        LIMIT %s;
                    """, (query_vector_str, query_vector_str, n_results * 2))
                    
                    results = []
                    for row in cur.fetchall():
                        doc_id, chunk_text, metadata, similarity = row
                        
                        # Filtrar por threshold
                        if similarity < similarity_threshold:
                            continue
                        
                        results.append({
                            'id': str(doc_id),
                            'document': chunk_text,
                            'content': chunk_text,
                            'metadata': metadata,
                            'similarity_score': float(similarity),
                            'distance': float(1 - similarity),
                            'rank': len(results) + 1
                        })
                        
                        if len(results) >= n_results:
                            break
                else:
                    # B√∫squeda manual con JSONB (menos eficiente pero funciona)
                    query_vector = json.dumps(query_embedding)
                    
                    cur.execute("""
                        WITH query_vec AS (
                            SELECT %s::jsonb as vec
                        )
                        SELECT 
                            id,
                            chunk_text,
                            embedding_vector,
                            metadata,
                            (
                                SELECT 1 - (
                                    SUM((ev.value::float * qv.value::float)) / 
                                    (
                                        SQRT(SUM(POW(ev.value::float, 2))) * 
                                        SQRT(SUM(POW(qv.value::float, 2)))
                                    )
                                )
                                FROM jsonb_array_elements_text(embedding_vector) WITH ORDINALITY ev(value, idx)
                                JOIN jsonb_array_elements_text((SELECT vec FROM query_vec)) WITH ORDINALITY qv(value, idx2)
                                    ON ev.idx = qv.idx2
                            ) as distance
                        FROM rag_embeddings
                        ORDER BY distance ASC
                        LIMIT %s * 2;
                    """, (query_vector, n_results))
                    
                    results = []
                    for row in cur.fetchall():
                        doc_id, chunk_text, embedding_json, metadata, distance = row
                        
                        # Convertir distancia a score de similitud
                        similarity_score = max(0, 1 - distance) if distance else 0
                        
                        # Filtrar por threshold
                        if similarity_score < similarity_threshold:
                            continue
                        
                        results.append({
                            'id': str(doc_id),
                            'document': chunk_text,
                            'content': chunk_text,
                            'metadata': metadata,
                            'similarity_score': similarity_score,
                            'distance': distance,
                            'rank': len(results) + 1
                        })
                        
                        if len(results) >= n_results:
                            break
                
                logger.info(f"üîç B√∫squeda completada: {len(results)} resultados")
                return results
                
        except Exception as e:
            logger.error(f"‚ùå Error en b√∫squeda: {e}")
            return []
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """
        Obtener estad√≠sticas de la colecci√≥n
        
        Returns:
            Estad√≠sticas detalladas
        """
        if not self.is_ready():
            return {"ready": False}
        
        try:
            with self.conn.cursor() as cur:
                # Contar documentos
                cur.execute("SELECT COUNT(*) FROM rag_embeddings;")
                total_docs = cur.fetchone()[0]
                
                # Obtener tama√±o de la tabla
                cur.execute("""
                    SELECT pg_size_pretty(pg_total_relation_size('rag_embeddings'));
                """)
                table_size = cur.fetchone()[0]
                
                return {
                    "ready": True,
                    "total_documents": total_docs,
                    "table_size": table_size,
                    "storage": "PostgreSQL",
                }
                
        except Exception as e:
            logger.error(f"‚ùå Error obteniendo estad√≠sticas: {e}")
            return {"ready": False, "error": str(e)}
    
    def check_document_exists(self, file_name: str, file_hash: str = None) -> bool:
        """
        Verificar si un documento ya tiene embeddings
        
        Args:
            file_name: Nombre del archivo
            file_hash: Hash opcional del contenido del archivo
            
        Returns:
            True si el documento ya fue procesado
        """
        if not self.is_ready():
            return False
        
        try:
            with self.conn.cursor() as cur:
                if file_hash:
                    # Verificar por hash (m√°s preciso)
                    cur.execute("""
                        SELECT COUNT(*) FROM rag_embeddings 
                        WHERE metadata->>'file_name' = %s 
                        AND metadata->>'file_hash' = %s
                    """, (file_name, file_hash))
                else:
                    # Verificar solo por nombre
                    cur.execute("""
                        SELECT COUNT(*) FROM rag_embeddings 
                        WHERE metadata->>'file_name' = %s
                    """, (file_name,))
                
                count = cur.fetchone()[0]
                return count > 0
                
        except Exception as e:
            logger.error(f"‚ùå Error verificando duplicado: {e}")
            return False
    
    def get_processed_files(self) -> List[Dict[str, Any]]:
        """
        Obtener lista de archivos procesados con estad√≠sticas
        
        Returns:
            Lista de archivos con chunks y metadatos
        """
        if not self.is_ready():
            return []
        
        try:
            with self.conn.cursor() as cur:
                cur.execute("""
                    SELECT 
                        metadata->>'file_name' as file_name,
                        metadata->>'file_hash' as file_hash,
                        metadata->>'type' as file_type,
                        COUNT(*) as chunks_count,
                        MIN(created_at) as first_processed,
                        MAX(created_at) as last_processed
                    FROM rag_embeddings
                    WHERE metadata->>'file_name' IS NOT NULL
                    GROUP BY metadata->>'file_name', metadata->>'file_hash', metadata->>'type'
                    ORDER BY last_processed DESC
                """)
                
                results = []
                for row in cur.fetchall():
                    results.append({
                        'file_name': row[0],
                        'file_hash': row[1],
                        'file_type': row[2],
                        'chunks_count': row[3],
                        'first_processed': row[4].isoformat() if row[4] else None,
                        'last_processed': row[5].isoformat() if row[5] else None,
                    })
                
                return results
                
        except Exception as e:
            logger.error(f"‚ùå Error obteniendo archivos procesados: {e}")
            return []
    
    def delete_document_embeddings(self, file_name: str, file_hash: str = None) -> int:
        """
        Eliminar todos los embeddings de un documento espec√≠fico
        
        Args:
            file_name: Nombre del archivo
            file_hash: Hash opcional del archivo
            
        Returns:
            Cantidad de embeddings eliminados
        """
        if not self.is_ready():
            return 0
        
        try:
            with self.conn.cursor() as cur:
                if file_hash:
                    cur.execute("""
                        DELETE FROM rag_embeddings 
                        WHERE metadata->>'file_name' = %s 
                        AND metadata->>'file_hash' = %s
                        RETURNING id
                    """, (file_name, file_hash))
                else:
                    cur.execute("""
                        DELETE FROM rag_embeddings 
                        WHERE metadata->>'file_name' = %s
                        RETURNING id
                    """, (file_name,))
                
                deleted_count = len(cur.fetchall())
                self.conn.commit()
                
                logger.info(f"üóëÔ∏è Eliminados {deleted_count} embeddings de '{file_name}'")
                return deleted_count
                
        except Exception as e:
            logger.error(f"‚ùå Error eliminando embeddings: {e}")
            self.conn.rollback()
            return 0

    def search_lexical(self, query: str, keywords: List[str] = None, 
                        n_results: int = 10) -> List[Dict[str, Any]]:
        """
        B√∫squeda l√©xica (full-text search) para coincidencias exactas de palabras.
        Complementa la b√∫squeda sem√°ntica para mejorar precisi√≥n.
        
        Args:
            query: Consulta original del usuario
            keywords: Lista de palabras clave (opcional)
            n_results: N√∫mero de resultados a retornar
        
        Returns:
            Lista de documentos con scores l√©xicos
        """
        if not self.is_ready():
            logger.error("‚ùå PostgreSQL no est√° listo para b√∫squeda l√©xica")
            return []
        
        try:
            # Extraer keywords si no se proporcionan
            if not keywords:
                import re
                stopwords = {
                    'el', 'la', 'los', 'las', 'de', 'del', 'y', 'en', 'un', 'una', 'que', 
                    'se', 'con', 'por', 'para', 'como', 'al', 'lo', 'su', 'sus', 'es', 'son',
                    'o', 'a', 'e', 'sobre', 'qu√©', 'cual', 'cu√°l', 'qui√©n', 'c√≥mo', 'd√≥nde',
                    'the', 'a', 'an', 'is', 'are', 'of', 'to', 'in', 'for', 'on', 'with',
                    'dice', 'menciona', 'habla', 'trata', 'explica', 'describe', 'documento'
                }
                words = re.findall(r'\b\w+\b', query.lower())
                keywords = [w for w in words if len(w) > 2 and w not in stopwords]
            
            if not keywords:
                logger.warning("‚ö†Ô∏è No se encontraron keywords para b√∫squeda l√©xica")
                return []
            
            logger.info(f"üî§ B√∫squeda l√©xica con keywords: {keywords[:10]}")
            
            with self.conn.cursor() as cur:
                # Construir condiciones de b√∫squeda
                # M√©todo 1: ILIKE para coincidencias parciales (m√°s flexible)
                conditions = []
                params = []
                
                for keyword in keywords[:15]:  # Limitar a 15 keywords
                    conditions.append("chunk_text ILIKE %s")
                    params.append(f"%{keyword}%")
                
                if not conditions:
                    return []
                
                # Tambi√©n buscar la consulta completa (boost alto si coincide)
                conditions.append("chunk_text ILIKE %s")
                params.append(f"%{query[:100]}%")  # Limitar longitud
                
                # Query con scoring basado en coincidencias
                query_sql = f"""
                    WITH keyword_matches AS (
                        SELECT 
                            id,
                            chunk_text,
                            metadata,
                            (
                                {' + '.join([f"(CASE WHEN chunk_text ILIKE %s THEN 1 ELSE 0 END)" for _ in keywords[:15]])}
                            ) as keyword_count,
                            CASE WHEN chunk_text ILIKE %s THEN 0.5 ELSE 0 END as exact_phrase_bonus
                        FROM rag_embeddings
                        WHERE {' OR '.join(conditions)}
                    )
                    SELECT 
                        id,
                        chunk_text,
                        metadata,
                        keyword_count,
                        exact_phrase_bonus,
                        (keyword_count::float / {len(keywords[:15])}) + exact_phrase_bonus as lexical_score
                    FROM keyword_matches
                    WHERE keyword_count > 0 OR exact_phrase_bonus > 0
                    ORDER BY lexical_score DESC, keyword_count DESC
                    LIMIT %s;
                """
                
                # Par√°metros: keywords para CASE, frase exacta para bonus, keywords para WHERE, frase para WHERE, l√≠mite
                all_params = []
                # Para los CASE statements
                for keyword in keywords[:15]:
                    all_params.append(f"%{keyword}%")
                # Para exact phrase bonus
                all_params.append(f"%{query[:100]}%")
                # Para las condiciones WHERE (ya est√°n en params)
                all_params.extend(params)
                # L√≠mite
                all_params.append(n_results * 2)
                
                cur.execute(query_sql, all_params)
                
                results = []
                for row in cur.fetchall():
                    doc_id, chunk_text, metadata, keyword_count, phrase_bonus, lexical_score = row
                    
                    results.append({
                        'id': str(doc_id),
                        'document': chunk_text,
                        'content': chunk_text,
                        'metadata': metadata,
                        'similarity_score': min(float(lexical_score), 1.0),
                        'lexical_score': float(lexical_score),
                        'keyword_matches': int(keyword_count),
                        'search_type': 'lexical',
                        'rank': len(results) + 1
                    })
                    
                    if len(results) >= n_results:
                        break
                
                logger.info(f"üî§ B√∫squeda l√©xica completada: {len(results)} resultados")
                return results
                
        except Exception as e:
            logger.error(f"‚ùå Error en b√∫squeda l√©xica: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_all_documents(self, limit: int = 1000) -> List[Dict[str, Any]]:
        """
        Obtener todos los documentos (para b√∫squeda l√©xica en memoria)
        
        Args:
            limit: N√∫mero m√°ximo de documentos
        
        Returns:
            Lista de documentos
        """
        if not self.is_ready():
            return []
        
        try:
            with self.conn.cursor() as cur:
                cur.execute("""
                    SELECT id, chunk_text, metadata
                    FROM rag_embeddings
                    LIMIT %s;
                """, (limit,))
                
                results = []
                for row in cur.fetchall():
                    doc_id, chunk_text, metadata = row
                    results.append({
                        'id': str(doc_id),
                        'document': chunk_text,
                        'content': chunk_text,
                        'metadata': metadata
                    })
                
                return results
                
        except Exception as e:
            logger.error(f"‚ùå Error obteniendo documentos: {e}")
            return []

    def reset_collection(self) -> int:
        """
        Eliminar todos los embeddings (CUIDADO)
        
        Returns:
            Cantidad de embeddings eliminados
        """
        if not self.is_ready():
            return 0
        
        try:
            with self.conn.cursor() as cur:
                # Contar antes de eliminar
                cur.execute("SELECT COUNT(*) FROM rag_embeddings;")
                count = cur.fetchone()[0]
                
                cur.execute("TRUNCATE TABLE rag_embeddings;")
                self.conn.commit()
                
                logger.info(f"üîÑ Tabla rag_embeddings vaciada: {count} embeddings eliminados")
                return count
                
        except Exception as e:
            logger.error(f"‚ùå Error vaciando tabla: {e}")
            if self.conn:
                self.conn.rollback()
            return 0
    
    def close(self):
        """Cerrar conexi√≥n"""
        if self.conn:
            self.conn.close()
            logger.info("‚úÖ Conexi√≥n PostgreSQL cerrada")
