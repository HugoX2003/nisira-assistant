"""
RAG Pipeline
===========

Pipeline principal que coordina todos los componentes del sistema RAG:
- Sincronizaci√≥n con Google Drive
- Procesamiento de documentos
- Generaci√≥n de embeddings
- Almacenamiento en ChromaDB
- B√∫squeda y generaci√≥n de respuestas
"""

import os
import re
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# Cargar variables de entorno al importar el m√≥dulo
load_dotenv()

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    LANGCHAIN_GOOGLE_AVAILABLE = True
except ImportError:
    LANGCHAIN_GOOGLE_AVAILABLE = False

from ..config import RAG_CONFIG, API_CONFIG
from ..drive_sync.drive_manager import GoogleDriveManager
from ..document_processing.pdf_processor import PDFProcessor
from ..document_processing.text_processor import TextProcessor
from ..embeddings.embedding_manager import EmbeddingManager
from ..vector_store.chroma_manager import ChromaManager

logger = logging.getLogger(__name__)

class RAGPipeline:
    """
    Pipeline principal del sistema RAG
    """
    
    def __init__(self):
        self.config = RAG_CONFIG
        
        # Inicializar componentes
        self.drive_manager = GoogleDriveManager()
        self.pdf_processor = PDFProcessor()
        self.text_processor = TextProcessor()
        self.embedding_manager = EmbeddingManager()
        self.chroma_manager = ChromaManager()
        
        # Configurar LLM para generaci√≥n
        self.llm = None
        self._initialize_llm()
        
        # Estad√≠sticas de procesamiento
        self.stats = {
            'documents_processed': 0,
            'chunks_created': 0,
            'embeddings_generated': 0,
            'last_sync': None,
            'last_processing': None
        }
    
    def _initialize_llm(self):
        """Inicializar modelo de lenguaje con soporte multi-proveedor"""
        from ..config import RAG_CONFIG
        
        provider = RAG_CONFIG["generation"]["provider"]
        logger.info(f"ü§ñ Inicializando LLM con proveedor: {provider}")
        
        try:
            if provider == "openrouter":
                self._initialize_openrouter_llm()
            elif provider == "groq":
                self._initialize_groq_llm()
            elif provider == "google":
                self._initialize_google_llm()
            else:
                logger.error(f"‚ùå Proveedor no soportado: {provider}")
                return
                
        except Exception as e:
            logger.error(f"‚ùå Error inicializando LLM {provider}: {e}")
            # Fallback a modo solo b√∫squeda
            self.llm = None
    
    def _initialize_openrouter_llm(self):
        """Inicializar OpenRouter LLM"""
        try:
            from langchain_openai import ChatOpenAI
        except ImportError:
            logger.error("‚ùå langchain_openai no disponible. Instalar con: pip install langchain-openai")
            return
            
        from ..config import RAG_CONFIG
        
        config = RAG_CONFIG["generation"]["openrouter"]
        api_key = config["api_key"]
        
        if not api_key or api_key == "your_openrouter_api_key_here":
            logger.warning("‚ùå API key de OpenRouter no configurada")
            return
        
        self.llm = ChatOpenAI(
            model=config["model"],
            openai_api_key=api_key,
            openai_api_base=config["base_url"],
            temperature=0.7,
            max_tokens=2048
        )
        logger.info(f"‚úÖ LLM OpenRouter inicializado: {config['model']}")
    
    def _initialize_groq_llm(self):
        """Inicializar Groq LLM"""
        try:
            from langchain_groq import ChatGroq
        except ImportError:
            logger.error("‚ùå langchain_groq no disponible. Instalar con: pip install langchain-groq")
            return
            
        from ..config import RAG_CONFIG
        
        config = RAG_CONFIG["generation"]["groq"]
        api_key = config["api_key"]
        
        if not api_key or api_key == "your_groq_api_key_here":
            logger.warning("‚ùå API key de Groq no configurada")
            return
        
        self.llm = ChatGroq(
            model=config["model"],
            groq_api_key=api_key,
            temperature=0.7,
            max_tokens=2048
        )
        logger.info(f"‚úÖ LLM Groq inicializado: {config['model']}")
    
    def _initialize_google_llm(self):
        """Inicializar Google Gemini LLM (m√©todo original)"""
        if not LANGCHAIN_GOOGLE_AVAILABLE:
            logger.warning("‚ùå LangChain Google no disponible")
            return

        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key:
            logger.warning("‚ùå API key de Google no configurada")
            return

        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=api_key,
            temperature=0.7,
            max_tokens=1024
        )
        logger.info("‚úÖ LLM Google Gemini inicializado")
    
    def is_ready(self) -> Dict[str, bool]:
        """Verificar estado de todos los componentes"""
        return {
            'drive_manager': self.drive_manager.is_authenticated(),
            'pdf_processor': self.pdf_processor.is_available(),
            'text_processor': self.text_processor.is_available(),
            'embedding_manager': self.embedding_manager.is_ready(),
            'chroma_manager': self.chroma_manager.is_ready(),
            'llm_available': self.llm is not None
        }
    
    def sync_and_process_documents(self, force_reprocess: bool = False) -> Dict[str, Any]:
        """Sincronizar documentos desde Google Drive y procesarlos"""
        logger.info("Iniciando sincronizaci√≥n y procesamiento de documentos")
        
        readiness = self.is_ready()
        
        if not readiness['drive_manager']:
            return {
                "success": False,
                "error": "Google Drive no est√° configurado correctamente"
            }
        
        if not readiness['embedding_manager'] or not readiness['chroma_manager']:
            return {
                "success": False,
                "error": "Sistema de embeddings o ChromaDB no est√° listo"
            }
        
        try:
            # 1. Sincronizar documentos desde Google Drive
            logger.info("Sincronizando documentos desde Google Drive...")
            sync_result = self.drive_manager.sync_documents()
            
            if not sync_result['success']:
                return {
                    "success": False,
                    "error": f"Error en sincronizaci√≥n: {sync_result.get('error', 'Unknown')}"
                }
            
            self.stats['last_sync'] = datetime.now().isoformat()
            
            # 2. Procesar documentos
            documents_to_process = []
            
            if force_reprocess:
                download_path = self.drive_manager.download_path
                if os.path.exists(download_path):
                    for file_name in os.listdir(download_path):
                        file_path = os.path.join(download_path, file_name)
                        if os.path.isfile(file_path):
                            documents_to_process.append(file_path)
            else:
                for file_info in sync_result.get('downloaded_files', []):
                    documents_to_process.append(file_info['path'])
            
            if not documents_to_process:
                logger.info("No hay documentos nuevos para procesar")
                return {
                    "success": True,
                    "message": "No hay documentos nuevos para procesar",
                    "sync_result": sync_result,
                    "processed_documents": 0
                }
            
            # 3. Procesar cada documento
            logger.info(f"Procesando {len(documents_to_process)} documentos...")
            
            all_chunks = []
            processing_results = []
            
            for doc_path in documents_to_process:
                try:
                    result = self.process_document(doc_path)
                    processing_results.append(result)
                    
                    if result['success']:
                        all_chunks.extend(result['chunks'])
                        self.stats['documents_processed'] += 1
                        logger.info(f"Procesado: {os.path.basename(doc_path)}")
                    else:
                        logger.error(f"Error procesando {os.path.basename(doc_path)}: {result.get('error', 'Unknown')}")
                        
                except Exception as e:
                    logger.error(f"Error inesperado procesando {doc_path}: {e}")
                    processing_results.append({
                        "success": False,
                        "file_path": doc_path,
                        "error": str(e)
                    })
            
            # 4. Generar embeddings
            if all_chunks:
                logger.info(f"Generando embeddings para {len(all_chunks)} chunks...")
                
                chunk_texts = [chunk['text'] for chunk in all_chunks]
                embeddings = self.embedding_manager.create_embeddings_batch(chunk_texts)
                
                valid_chunks = []
                valid_embeddings = []
                
                for chunk, embedding in zip(all_chunks, embeddings):
                    if embedding is not None:
                        valid_chunks.append(chunk)
                        valid_embeddings.append(embedding)
                
                self.stats['chunks_created'] += len(valid_chunks)
                self.stats['embeddings_generated'] += len(valid_embeddings)
                
                # 5. Almacenar en ChromaDB
                if valid_chunks:
                    logger.info(f"Almacenando {len(valid_chunks)} chunks en ChromaDB...")
                    
                    storage_success = self.chroma_manager.add_documents(
                        valid_chunks, 
                        valid_embeddings
                    )
                    
                    if not storage_success:
                        logger.error("Error almacenando en ChromaDB")
                        return {
                            "success": False,
                            "error": "Error almacenando documentos en base de datos vectorial"
                        }
            
            self.stats['last_processing'] = datetime.now().isoformat()
            
            successful_docs = len([r for r in processing_results if r['success']])
            failed_docs = len(processing_results) - successful_docs
            
            result = {
                "success": True,
                "sync_result": sync_result,
                "processing_summary": {
                    "total_documents": len(documents_to_process),
                    "successful": successful_docs,
                    "failed": failed_docs,
                    "total_chunks": len(all_chunks),
                    "valid_chunks": len(valid_chunks) if all_chunks else 0
                },
                "processing_results": processing_results,
                "stats": self.stats
            }
            
            logger.info(f"Procesamiento completado: {successful_docs}/{len(documents_to_process)} documentos exitosos")
            
            return result
            
        except Exception as e:
            logger.error(f"Error en sincronizaci√≥n y procesamiento: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def process_document(self, file_path: str) -> Dict[str, Any]:
        """Procesar un documento individual"""
        if not os.path.exists(file_path):
            return {
                "success": False,
                "error": f"Archivo no encontrado: {file_path}"
            }
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        try:
            if file_ext == '.pdf':
                if not self.pdf_processor.is_available():
                    return {
                        "success": False,
                        "error": "Procesador PDF no disponible"
                    }
                result = self.pdf_processor.process_pdf(file_path)
            
            elif file_ext in self.text_processor.get_supported_formats():
                result = self.text_processor.process_document(file_path)
            
            else:
                return {
                    "success": False,
                    "error": f"Formato no soportado: {file_ext}"
                }
            
            return result
            
        except Exception as e:
            logger.error(f"Error procesando documento {file_path}: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_path": file_path
            }
    
    def query(self, question: str, top_k: int = 5, include_generation: bool = True) -> Dict[str, Any]:
        """Realizar consulta RAG completa con optimizaci√≥n para citas"""
        logger.info(f"Procesando consulta: {question[:100]}...")
        
        if not question.strip():
            return {
                "success": False,
                "error": "Pregunta vac√≠a"
            }
        
        try:
            # 1. Detectar si es una consulta sobre citas espec√≠ficas
            is_citation_query, enhanced_query = self._enhance_citation_query(question)
            
            # 2. Usar la consulta mejorada para embeddings
            search_query = enhanced_query if is_citation_query else question
            query_embedding = self.embedding_manager.create_embedding(search_query)
            
            if query_embedding is None:
                return {
                    "success": False,
                    "error": "No se pudo crear embedding de la consulta"
                }
            
            # 2. B√öSQUEDA H√çBRIDA S√öPER AGRESIVA
            relevant_docs = self._hybrid_search(question, enhanced_query, query_embedding, top_k)
            
            if not relevant_docs:
                return {
                    "success": True,
                    "question": question,
                    "relevant_documents": [],
                    "answer": "No se encontraron documentos relevantes para responder la pregunta.",
                    "sources": []
                }
            
            # 3. Preparar contexto
            context_parts = []
            sources = []
            
            for i, doc in enumerate(relevant_docs):
                context_parts.append(f"Documento {i+1}:\\n{doc.get('content', doc.get('document', ''))}")
                
                metadata = doc['metadata']
                source_info = {
                    "rank": doc.get('rank', i + 1),
                    "similarity_score": doc.get('similarity_score', 0),
                    "file_name": metadata.get('source', metadata.get('document', metadata.get('file_name', 'unknown'))),
                    "chunk_id": metadata.get('chunk_id', 0)
                }
                
                if 'page' in metadata:
                    source_info['page'] = metadata['page']
                
                sources.append(source_info)
            
            context = "\\n\\n".join(context_parts)
            
            # 4. Generar respuesta si est√° habilitado
            answer = None
            
            if include_generation and self.llm:
                try:
                    prompt = self._create_rag_prompt(question, context)
                    response = self.llm.invoke(prompt)
                    answer = response.content if hasattr(response, 'content') else str(response)
                    
                except Exception as e:
                    logger.error(f"Error generando respuesta: {e}")
                    answer = f"Error generando respuesta autom√°tica: {e}"
            
            result = {
                "success": True,
                "question": question,
                "relevant_documents": relevant_docs,
                "context": context,
                "sources": sources,
                "answer": answer,
                "generation_used": answer is not None
            }
            
            logger.info(f"Consulta procesada: {len(relevant_docs)} documentos encontrados")
            
            return result
            
        except Exception as e:
            logger.error(f"Error en consulta RAG: {e}")
            return {
                "success": False,
                "error": str(e),
                "question": question
            }
    
    def _create_rag_prompt(self, question: str, context: str) -> str:
        """Crear prompt para generaci√≥n RAG con formato Markdown mejorado"""
        prompt = f"""Eres un asistente acad√©mico especializado en an√°lisis de textos y documentos peruanos. Tu trabajo es encontrar y explicar informaci√≥n relevante bas√°ndote en los documentos proporcionados.

CONTEXTO DE DOCUMENTOS:
{context}

PREGUNTA DEL USUARIO: {question}

INSTRUCCIONES:
1. Analiza profundamente el contenido buscando conceptos e ideas relacionadas
2. S√© interpretativo: relaciona conceptualmente informaci√≥n aunque no use palabras exactas
3. Explora conexiones entre diferentes documentos y partes del texto
4. Cita espec√≠ficamente las fuentes y documentos consultados
5. Combina informaci√≥n de m√∫ltiples fuentes cuando sea relevante

FORMATO DE RESPUESTA - USA MARKDOWN ESTRUCTURADO:

### Respuesta Principal
Explica la informaci√≥n encontrada de manera clara y detallada.

**Conceptos clave:** Usa negrita para t√©rminos importantes
*√ânfasis espec√≠fico:* Usa cursiva para ideas relevantes  
`Fuentes`: Usa c√≥digo para nombres de documentos

#### Informaci√≥n Detallada
1. **Primera idea importante:** Explicaci√≥n detallada
2. **Segunda idea relevante:** M√°s informaci√≥n
3. **Conexiones encontradas:** Relaciones entre conceptos

> Si hay citas textuales importantes, ponlas en formato de cita

#### Fuentes Consultadas
- `Documento 1`: Informaci√≥n espec√≠fica encontrada
- `Documento 2`: Datos relevantes identificados

---

Responde de manera acad√©mica, bien estructurada y utilizando el formato Markdown apropiadamente para mejorar la legibilidad:"""
        
        return prompt
    
    def get_system_status(self) -> Dict[str, Any]:
        """Obtener estado completo del sistema RAG"""
        return {
            "readiness": self.is_ready(),
            "stats": self.stats,
            "drive_status": self.drive_manager.get_sync_status(),
            "embedding_stats": self.embedding_manager.get_stats(),
            "chroma_stats": self.chroma_manager.get_collection_stats(),
            "supported_formats": {
                "pdf": self.pdf_processor.get_supported_formats(),
                "text": self.text_processor.get_supported_formats()
            }
        }
    
    def reset_system(self) -> Dict[str, Any]:
        """Resetear completamente el sistema"""
        logger.warning("Iniciando reset completo del sistema RAG")
        
        try:
            chroma_reset = self.chroma_manager.reset_collection()
            self.embedding_manager.clear_cache()
            
            self.stats = {
                'documents_processed': 0,
                'chunks_created': 0,
                'embeddings_generated': 0,
                'last_sync': None,
                'last_processing': None
            }
            
            result = {
                "success": chroma_reset,
                "chroma_reset": chroma_reset,
                "cache_cleared": True,
                "stats_reset": True
            }
            
            if chroma_reset:
                logger.info("Sistema RAG reseteado completamente")
            else:
                logger.error("Error reseteando sistema RAG")
            
            return result
            
        except Exception as e:
            logger.error(f"Error en reset del sistema: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _enhance_citation_query(self, question: str) -> Tuple[bool, str]:
        """
        Detectar y mejorar consultas sobre citas bibliogr√°ficas espec√≠ficas
        
        Returns:
            Tuple[bool, str]: (is_citation_query, enhanced_query)
        """
        import re
        
        # Patrones para detectar consultas sobre citas
        citation_patterns = [
            r'([A-Z][a-z√°√©√≠√≥√∫√±√º]+)\s*\(\s*(\d{4})\s*\)',  # Arias(2020)
            r'([A-Z][a-z√°√©√≠√≥√∫√±√º]+)\s+(\d{4})',           # Arias 2020
            r'([A-Z][a-z√°√©√≠√≥√∫√±√º]+)\s+et\s+al\.\s*\(\s*(\d{4})\s*\)'  # Arias et al.(2020)
        ]
        
        enhanced_query = question
        is_citation_query = False
        
        for pattern in citation_patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            if matches:
                is_citation_query = True
                for match in matches:
                    if len(match) == 2:  # (autor, a√±o)
                        author, year = match
                        # A√±adir contexto para mejorar la b√∫squeda
                        enhanced_query += f" {author} {year} seg√∫n menciona dice refiere cita bibliogr√°fica investigaci√≥n estudio an√°lisis variables definici√≥n teor√≠a"
                
        return is_citation_query, enhanced_query
    
    def _hybrid_search(self, original_query: str, enhanced_query: str, query_embedding, top_k: int) -> List[Dict[str, Any]]:
        """
        B√öSQUEDA H√çBRIDA MEJORADA
        Combina m√∫ltiples estrategias con pesos optimizados para m√°xima precisi√≥n
        """
        all_results = []
        seen_ids = set()
        
        logger.info(f"üîç Iniciando b√∫squeda h√≠brida optimizada para: '{original_query}'")
        
        # Obtener configuraci√≥n de pesos
        config = self.config.get("retrieval", {})
        semantic_weight = config.get("semantic_weight", 0.6)
        lexical_weight = config.get("lexical_weight", 0.4)
        max_results = config.get("top_k", 15)
        
        # 1. B√öSQUEDA SEM√ÅNTICA MEJORADA
        try:
            semantic_docs = self.chroma_manager.search_similar(
                query_embedding, 
                n_results=max_results,
                similarity_threshold=config.get("similarity_threshold", 0.005)
            )
            for i, doc in enumerate(semantic_docs):
                # Usar doc_id del metadata como ID √∫nico
                doc_id = doc.get('metadata', {}).get('doc_id', f"semantic_{len(all_results)}")
                if doc_id not in seen_ids:
                    # Aplicar peso sem√°ntico y boost por posici√≥n
                    position_boost = 1.0 - (i * 0.05)  # Reduce 5% por posici√≥n
                    weighted_score = doc.get('similarity_score', 0) * semantic_weight * position_boost
                    
                    doc['id'] = doc_id
                    doc['search_type'] = 'semantic'
                    doc['weighted_score'] = weighted_score
                    doc['original_score'] = doc.get('similarity_score', 0)
                    all_results.append(doc)
                    seen_ids.add(doc_id)
            logger.info(f"üìä B√∫squeda sem√°ntica: {len(semantic_docs)} documentos (peso: {semantic_weight})")
        except Exception as e:
            logger.warning(f"Error en b√∫squeda sem√°ntica: {e}")
        
        # 2. B√öSQUEDA ADICIONAL (DESHABILITADA TEMPORALMENTE)
        # La b√∫squeda por texto tiene conflicto de dimensiones
        try:
            pass  # Placeholder - b√∫squeda lexical deshabilitada temporalmente
        except Exception as e:
            logger.warning(f"Error en b√∫squeda lexical: {e}")
        
        # 3. B√öSQUEDA POR METADATOS (nombres de archivos y fuentes)
        try:
            metadata_docs = self._enhanced_metadata_search(original_query, max_results // 2)
            for doc in metadata_docs:
                if doc['id'] not in seen_ids:
                    # Metadata tiene peso fijo alto para coincidencias exactas
                    doc['search_type'] = 'metadata'
                    doc['weighted_score'] = doc.get('similarity_score', 1.0)  # Score alto para metadata
                    doc['original_score'] = doc.get('similarity_score', 1.0)
                    all_results.append(doc)
                    seen_ids.add(doc['id'])
            logger.info(f"üìä B√∫squeda por metadatos: {len(metadata_docs)} documentos")
        except Exception as e:
            logger.warning(f"Error en b√∫squeda por metadatos: {e}")
        
        # 4. B√öSQUEDA DE EXPANSI√ìN (si pocos resultados)
        if len(all_results) < top_k:
            logger.info("ÔøΩ Expandiendo b√∫squeda con t√©rminos relacionados...")
            try:
                expanded_docs = self._expansion_search(original_query, top_k)
                for doc in expanded_docs:
                    if doc['id'] not in seen_ids:
                        doc['search_type'] = 'expansion'
                        doc['weighted_score'] = doc.get('similarity_score', 0) * 0.3  # Peso bajo
                        doc['original_score'] = doc.get('similarity_score', 0)
                        all_results.append(doc)
                        seen_ids.add(doc['id'])
                logger.info(f"üìä B√∫squeda expandida: {len(expanded_docs)} documentos adicionales")
            except Exception as e:
                logger.warning(f"Error en b√∫squeda expandida: {e}")
        
        # 5. ORDENAR POR WEIGHTED SCORE
        all_results.sort(key=lambda x: x.get('weighted_score', 0), reverse=True)
        
        # 6. DIVERSIFICACI√ìN (evitar documentos muy similares)
        diverse_results = self._diversify_results(all_results, config.get("diversity_threshold", 0.4))
        
        logger.info(f"‚úÖ B√∫squeda h√≠brida completada: {len(diverse_results)} documentos √∫nicos (de {len(all_results)} totales)")
        
        return diverse_results[:top_k]
    
    def _extract_keywords(self, query: str) -> List[str]:
        """Extraer palabras clave de la consulta"""
        import re
        
        # Limpiar y extraer palabras
        words = re.findall(r'\b\w+\b', query.lower())
        
        # Filtrar stopwords comunes
        stopwords = {'el', 'la', 'de', 'que', 'y', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 
                    'da', 'su', 'por', 'son', 'con', 'para', 'como', 'las', 'del', 'los', 'una', 
                    'est√°', 'qu√©', 'dice', 'sobre', 'qui√©n', 'c√≥mo', 'cu√°l', 'd√≥nde'}
        
        keywords = [word for word in words if len(word) > 2 and word not in stopwords]
        
        return keywords
    
    def _search_by_filename(self, keywords: List[str], top_k: int) -> List[Dict[str, Any]]:
        """Buscar por coincidencias en nombres de archivos"""
        results = []
        
        try:
            collection = self.chroma_manager.collection
            all_data = collection.get()
            
            ids = all_data.get('ids', [])
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            
            for i, metadata in enumerate(metadatas):
                if i >= len(ids) or i >= len(documents):
                    continue
                    
                source = metadata.get('source', '').lower()
                
                # Calcular score basado en coincidencias de palabras clave
                score = 0
                for keyword in keywords:
                    if keyword in source:
                        score += 1
                
                if score > 0:
                    results.append({
                        'id': ids[i],
                        'document': documents[i],
                        'metadata': metadata,
                        'similarity_score': score / len(keywords),  # Normalizar score
                        'rank': len(results) + 1
                    })
            
            # Ordenar por score descendente
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            
        except Exception as e:
            logger.error(f"Error en b√∫squeda por filename: {e}")
        
        return results[:top_k]
    
    def _lexical_search(self, keywords: List[str], top_k: int) -> List[Dict[str, Any]]:
        """B√∫squeda lexical en el contenido de los documentos"""
        results = []
        
        try:
            collection = self.chroma_manager.collection
            all_data = collection.get()
            
            ids = all_data.get('ids', [])
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            
            for i, document in enumerate(documents):
                if i >= len(ids) or i >= len(metadatas):
                    continue
                    
                content = document.lower()
                
                # Calcular score basado en frecuencia de palabras clave
                score = 0
                for keyword in keywords:
                    score += content.count(keyword)
                
                if score > 0:
                    results.append({
                        'id': ids[i],
                        'document': document,
                        'metadata': metadatas[i],
                        'similarity_score': min(score / 10, 1.0),  # Normalizar y limitar a 1.0
                        'rank': len(results) + 1
                    })
            
            # Ordenar por score descendente
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            
        except Exception as e:
            logger.error(f"Error en b√∫squeda lexical: {e}")
        
        return results[:top_k]
    
    def _smart_lexical_search(self, keywords: List[str], original_query: str, top_k: int) -> List[Dict[str, Any]]:
        """B√∫squeda lexical inteligente con coincidencias parciales y fuzzy matching"""
        results = []
        
        try:
            collection = self.chroma_manager.collection
            all_data = collection.get(include=['documents', 'metadatas'])
            
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            ids = all_data.get('ids', [])
            
            # Expandir keywords con variaciones
            expanded_keywords = set(keywords)
            for kw in keywords:
                # Agregar plurales y singulares b√°sicos
                if kw.endswith('s'):
                    expanded_keywords.add(kw[:-1])
                else:
                    expanded_keywords.add(kw + 's')
                    
                # Agregar variaciones de capitalizaci√≥n
                expanded_keywords.add(kw.upper())
                expanded_keywords.add(kw.capitalize())
            
            for i, document in enumerate(documents):
                if i >= len(ids) or i >= len(metadatas):
                    continue
                
                doc_lower = document.lower()
                score = 0
                matches = 0
                
                # Calcular score basado en coincidencias
                for keyword in expanded_keywords:
                    kw_lower = keyword.lower()
                    if kw_lower in doc_lower:
                        # Boost para coincidencias exactas
                        count = doc_lower.count(kw_lower)
                        score += count * (0.2 if keyword in keywords else 0.1)
                        matches += count
                
                # Boost adicional si el documento contiene la frase completa
                if original_query.lower() in doc_lower:
                    score += 0.5
                    matches += 1
                
                if score > 0:
                    results.append({
                        'id': ids[i],
                        'content': document,
                        'metadata': metadatas[i],
                        'similarity_score': min(score, 1.0),  # Normalizar a m√°ximo 1.0
                        'match_count': matches
                    })
            
            # Ordenar por score y luego por n√∫mero de matches
            results.sort(key=lambda x: (x['similarity_score'], x['match_count']), reverse=True)
            
        except Exception as e:
            logger.error(f"Error en b√∫squeda lexical inteligente: {e}")
        
        return results[:top_k]
    
    def _enhanced_metadata_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """B√∫squeda mejorada en metadatos y nombres de archivos"""
        results = []
        
        try:
            collection = self.chroma_manager.collection
            all_data = collection.get(include=['documents', 'metadatas'])
            
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            ids = all_data.get('ids', [])
            
            query_words = set(re.findall(r'\b\w+\b', query.lower()))
            
            for i, metadata in enumerate(metadatas):
                if i >= len(ids):
                    continue
                
                score = 0
                
                # Buscar en source/filename
                source = metadata.get('source', '').lower()
                if source:
                    source_words = set(re.findall(r'\b\w+\b', source))
                    common_words = query_words.intersection(source_words)
                    if common_words:
                        score += len(common_words) / len(query_words) * 0.8
                
                # Buscar en document field
                document_field = metadata.get('document', '').lower()
                if document_field:
                    doc_words = set(re.findall(r'\b\w+\b', document_field))
                    common_words = query_words.intersection(doc_words)
                    if common_words:
                        score += len(common_words) / len(query_words) * 0.6
                
                # B√∫squeda exacta en t√≠tulos/fuentes
                if query.lower() in source or query.lower() in document_field:
                    score += 0.9
                
                if score > 0:
                    results.append({
                        'id': ids[i],
                        'content': documents[i] if i < len(documents) else '',
                        'metadata': metadata,
                        'similarity_score': min(score, 1.0)
                    })
            
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            
        except Exception as e:
            logger.error(f"Error en b√∫squeda de metadatos: {e}")
        
        return results[:top_k]
    
    def _expansion_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """B√∫squeda con expansi√≥n de t√©rminos y conceptos relacionados"""
        results = []
        
        # Diccionario de expansi√≥n de t√©rminos acad√©micos
        expansion_dict = {
            'derecho': ['derechos', 'legal', 'jur√≠dico', 'justicia', 'normativo'],
            'mugre': ['suciedad', 'contaminaci√≥n', 'higiene', 'limpieza', 'sanitario'],
            'sat√°n': ['diablo', 'demonio', 'mal', 'infernal', 'diab√≥lico'],
            'crucificado': ['crucifixi√≥n', 'cruz', 'crucificar', 'martirio'],
            'democracia': ['democr√°tico', 'democratizaci√≥n', 'participaci√≥n', 'electoral'],
            'pol√≠tica': ['pol√≠tico', 'gobierno', 'estado', 'poder', 'gesti√≥n'],
            'regionalizaci√≥n': ['regional', 'descentralizaci√≥n', 'territorio', 'local'],
            'racismo': ['racial', 'discriminaci√≥n', '√©tnico', 'prejuicio'],
            'violencia': ['violento', 'agresi√≥n', 'conflicto', 'guerra']
        }
        
        try:
            # Extraer t√©rminos clave de la consulta
            query_terms = re.findall(r'\b\w+\b', query.lower())
            expanded_terms = set(query_terms)
            
            # Expandir con t√©rminos relacionados
            for term in query_terms:
                if term in expansion_dict:
                    expanded_terms.update(expansion_dict[term])
            
            # Buscar con t√©rminos expandidos
            collection = self.chroma_manager.collection
            all_data = collection.get(include=['documents', 'metadatas'])
            
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            ids = all_data.get('ids', [])
            
            for i, document in enumerate(documents):
                if i >= len(ids) or i >= len(metadatas):
                    continue
                
                doc_lower = document.lower()
                score = 0
                
                # Calcular relevancia basada en t√©rminos expandidos
                for term in expanded_terms:
                    if term in doc_lower:
                        # T√©rminos originales tienen m√°s peso
                        weight = 0.3 if term in query_terms else 0.1
                        count = doc_lower.count(term)
                        score += count * weight
                
                if score > 0:
                    results.append({
                        'id': ids[i],
                        'content': document,
                        'metadata': metadatas[i],
                        'similarity_score': min(score / len(expanded_terms), 1.0)
                    })
            
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            
        except Exception as e:
            logger.error(f"Error en b√∫squeda expandida: {e}")
        
        return results[:top_k]
    
    def _diversify_results(self, results: List[Dict[str, Any]], diversity_threshold: float) -> List[Dict[str, Any]]:
        """Diversificar resultados para evitar documentos muy similares"""
        if not results:
            return results
        
        diverse_results = [results[0]]  # Siempre incluir el mejor resultado
        
        for result in results[1:]:
            is_diverse = True
            result_content = result.get('content', '').lower()
            
            for existing in diverse_results:
                existing_content = existing.get('content', '').lower()
                
                # Calcular similitud simple basada en palabras comunes
                result_words = set(re.findall(r'\b\w+\b', result_content))
                existing_words = set(re.findall(r'\b\w+\b', existing_content))
                
                if result_words and existing_words:
                    similarity = len(result_words.intersection(existing_words)) / len(result_words.union(existing_words))
                    if similarity > diversity_threshold:
                        is_diverse = False
                        break
            
            if is_diverse:
                diverse_results.append(result)
        
        return diverse_results
    
    def _desperate_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """B√∫squeda desesperada - devuelve cualquier contenido relacionado"""
        results = []
        
        try:
            collection = self.chroma_manager.collection
            all_data = collection.get()
            
            ids = all_data.get('ids', [])
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            
            query_words = set(re.findall(r'\b\w+\b', query.lower()))
            
            for i, document in enumerate(documents):
                if i >= len(ids) or i >= len(metadatas):
                    continue
                    
                doc_words = set(re.findall(r'\b\w+\b', document.lower()))
                
                # Calcular intersecci√≥n de palabras
                common_words = query_words.intersection(doc_words)
                
                if common_words:
                    score = len(common_words) / len(query_words)
                    results.append({
                        'id': ids[i],
                        'document': document,
                        'metadata': metadatas[i],
                        'similarity_score': score,
                        'rank': len(results) + 1
                    })
            
            # Ordenar por score descendente
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            
        except Exception as e:
            logger.error(f"Error en b√∫squeda desesperada: {e}")
        
        return results[:top_k]
    
    def _rank_hybrid_results(self, results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """Reordenar resultados h√≠bridos por relevancia total"""
        
        # Asignar pesos por tipo de b√∫squeda
        type_weights = {
            'semantic': 1.0,
            'metadata': 0.8,
            'lexical': 0.6,
            'desperate': 0.3
        }
        
        for result in results:
            search_type = result.get('search_type', 'desperate')
            original_score = result.get('similarity_score', 0)
            weight = type_weights.get(search_type, 0.3)
            
            # Calcular score final ponderado
            result['final_score'] = original_score * weight
        
        # Ordenar por score final
        results.sort(key=lambda x: x.get('final_score', 0), reverse=True)
        
        return results