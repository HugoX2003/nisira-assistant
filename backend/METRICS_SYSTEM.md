# üìä Sistema de M√©tricas del Asistente RAG

## ‚úÖ Cambios Realizados

### **RAGAS Eliminado** ‚ùå
- ‚ùå Dependencia de `ragas` removida
- ‚ùå Ya no requiere API keys (OpenAI/OpenRouter)
- ‚ùå Sin problemas de compatibilidad

### **Sistema Personalizado Implementado** ‚úÖ
- ‚úÖ Evaluaci√≥n sin dependencias externas
- ‚úÖ M√©tricas calculadas localmente
- ‚úÖ Sin costos de API

---

## üìà M√©tricas Disponibles

### üöÄ **M√©tricas de Rendimiento** (Ya funcionando)

#### 1. **Tiempo de Respuesta Promedio** (`avgResponseTime`)
- **Qu√© mide**: Latencia total desde pregunta hasta respuesta completa
- **Fuente**: Base de datos `QueryMetrics`
- **Campo**: `total_latency`

#### 2. **Velocidad de Procesamiento** (`timeToFirstToken`)
- **Qu√© mide**: Tiempo hasta que el usuario ve el primer token de la respuesta
- **Fuente**: Base de datos `QueryMetrics`
- **Campo**: `time_to_first_token`
- **Importancia**: Usuario percibe rapidez si este valor es bajo

#### 3. **Tiempo de Resoluci√≥n de Consultas Complejas** (`complexQueryTime`)
- **Qu√© mide**: Latencia promedio solo para consultas clasificadas como complejas
- **Fuente**: Base de datos `QueryMetrics` WHERE `is_complex_query=True`
- **Campo**: `total_latency`
- **Clasificaci√≥n**: Autom√°tica basada en longitud, palabras clave, m√∫ltiples preguntas

---

### üéØ **M√©tricas de Precisi√≥n** (Sistema Personalizado)

#### 4. **Precision@k** (`precisionAtK`)
- **Qu√© mide**: ¬øCu√°ntos de los K documentos recuperados fueron realmente √∫tiles?
- **Rango**: 0.0 - 1.0 (mayor es mejor)
- **M√©todo**: 
  - Calcula overlap sem√°ntico (Jaccard similarity) entre cada contexto y la respuesta
  - Si overlap > 20%, el documento se considera relevante
  - `Precision@k = (documentos relevantes) / k`
- **Ejemplo**: Si k=5 y 4 documentos fueron √∫tiles ‚Üí Precision@5 = 0.80

#### 5. **Recall@k** (`recallAtK`)
- **Qu√© mide**: ¬øLa respuesta cubre informaci√≥n de m√∫ltiples contextos relevantes?
- **Rango**: 0.0 - 1.0 (mayor es mejor)
- **M√©todo**:
  - Extrae frases clave de cada contexto (n-gramas de 3 palabras)
  - Verifica cu√°ntos contextos tienen frases presentes en la respuesta
  - `Recall@k = (contextos cubiertos) / k`
- **Ejemplo**: Si k=5 y la respuesta usa info de 3 contextos ‚Üí Recall@5 = 0.60

#### 6. **Faithfulness / Fidelidad** (`faithfulness`)
- **Qu√© mide**: ¬øLa respuesta est√° respaldada por el contexto? (sin inventar informaci√≥n)
- **Rango**: 0.0 - 1.0 (mayor es mejor)
- **M√©todo**:
  - Divide la respuesta en oraciones
  - Para cada oraci√≥n, extrae palabras clave
  - Verifica si >60% de las palabras clave est√°n en el contexto
  - `Faithfulness = (oraciones respaldadas) / (total oraciones)`
- **Ejemplo**: Si 8 de 10 oraciones est√°n respaldadas ‚Üí Faithfulness = 0.80

#### 7. **Tasa de Alucinaci√≥n** (`hallucinationRate`)
- **Qu√© mide**: ¬øCu√°nta informaci√≥n invent√≥ el sistema?
- **Rango**: 0.0 - 1.0 (menor es mejor)
- **F√≥rmula**: `Hallucination Rate = 1.0 - Faithfulness`
- **Ejemplo**: Si Faithfulness = 0.80 ‚Üí Hallucination Rate = 0.20 (20% inventado)

#### 8. **Answer Relevancy** (`answer_relevancy`)
- **Qu√© mide**: ¬øLa respuesta aborda la pregunta planteada?
- **Rango**: 0.0 - 1.0 (mayor es mejor)
- **M√©todo**:
  - Extrae palabras clave de la pregunta
  - Verifica cu√°ntas keywords est√°n en la respuesta
  - Bonus si la respuesta tiene longitud razonable (20-300 palabras)
- **Ejemplo**: Si 7 de 10 keywords est√°n presentes ‚Üí Relevancy ‚âà 0.70

#### 9. **WER (Word Error Rate)** (`wer`) - NUEVO ‚≠ê
- **Qu√© mide**: Calidad de generaci√≥n de texto comparado con referencia
- **Rango**: 0.0 - ‚àû (menor es mejor, 0.0 es perfecto)
- **M√©todo**: Distancia de Levenshtein entre texto generado y referencia
- **F√≥rmula**: `WER = (Sustituciones + Inserciones + Eliminaciones) / Palabras en referencia`
- **Uso**: Solo cuando hay ground truth disponible
- **Ejemplo**: Si hay 5 errores en 50 palabras ‚Üí WER = 0.10

---

## üîß Implementaci√≥n T√©cnica

### Archivos Modificados

1. **`api/custom_evaluator.py`** (NUEVO)
   - Clase `CustomMetricsEvaluator`
   - M√©todos: `calculate_precision_at_k()`, `calculate_recall_at_k()`, `calculate_faithfulness()`, `calculate_wer()`
   - Sin dependencias externas, solo Python est√°ndar

2. **`api/metrics_tracker.py`** (MODIFICADO)
   - Importa `custom_evaluator` en lugar de `ragas_evaluator`
   - Guarda m√©tricas personalizadas en `RAGASMetrics` (nombre legacy)
   - Incluye WER cuando hay ground truth

3. **`api/models.py`** (MODIFICADO)
   - `RAGASMetrics` ahora incluye campo `wer_score`
   - Campos con defaults para evitar errores
   - Comentarios actualizados

4. **`api/admin_views.py`** (MODIFICADO)
   - Endpoint `/api/admin/metrics/` actualizado
   - Metadata indica `dataSource: "real_database_custom_metrics"`

### Base de Datos

```sql
-- Nueva columna agregada
ALTER TABLE api_ragasmetrics 
ADD COLUMN wer_score FLOAT NULL;

-- Campos actualizados con defaults
ALTER TABLE api_ragasmetrics 
ALTER COLUMN precision_at_k SET DEFAULT 0.0,
ALTER COLUMN recall_at_k SET DEFAULT 0.0,
ALTER COLUMN faithfulness_score SET DEFAULT 0.0,
ALTER COLUMN answer_relevancy SET DEFAULT 0.0,
ALTER COLUMN hallucination_rate SET DEFAULT 0.0;
```

---

## üìä C√≥mo Ver las M√©tricas

### 1. **Panel de Administraci√≥n** (Frontend)
```
http://localhost:3000/admin
Usuario: admin
Pesta√±a: üìä M√©tricas
```

**Visualizaci√≥n:**
- ‚è±Ô∏è Tiempo de Respuesta Promedio: `2.34s`
- ‚ö° Velocidad de Procesamiento: `0.45s`
- üß† Tiempo Consultas Complejas: `3.12s`
- üéØ Precision@k: `85%`
- üìã Recall@k: `78%`
- ‚úÖ Fidelidad (Faithfulness): `92%`
- ‚ö†Ô∏è Tasa de Alucinaci√≥n: `8%`

### 2. **API Endpoint**
```http
GET /api/admin/metrics/
Authorization: Bearer <JWT_TOKEN>
```

**Respuesta:**
```json
{
  "success": true,
  "metrics": {
    "performance": {
      "avgResponseTime": 2.34,
      "timeToFirstToken": 0.45,
      "complexQueryTime": 3.12,
      "totalQueries": 156
    },
    "precision": {
      "precisionAtK": 0.85,
      "recallAtK": 0.78,
      "hallucinationRate": 0.08,
      "faithfulness": 0.92
    },
    "metadata": {
      "lastUpdated": "2025-11-14T03:50:00",
      "dataSource": "real_database_custom_metrics",
      "kValue": 5,
      "isRealData": true
    }
  }
}
```

### 3. **Base de Datos Directamente**
```python
from api.models import QueryMetrics, RAGASMetrics

# Ver m√©tricas de rendimiento
QueryMetrics.objects.all().aggregate(
    avg_latency=Avg('total_latency'),
    avg_ttft=Avg('time_to_first_token')
)

# Ver m√©tricas de precisi√≥n
RAGASMetrics.objects.all().aggregate(
    avg_precision=Avg('precision_at_k'),
    avg_faithfulness=Avg('faithfulness_score'),
    avg_wer=Avg('wer_score')
)
```

---

## üß™ C√≥mo se Calculan (Ejemplo Real)

### Escenario: Usuario pregunta sobre ISO 27001

**Pregunta:** "¬øQu√© es ISO 27001 y cu√°les son sus controles principales?"

**Respuesta generada:** "ISO 27001 es un est√°ndar internacional de seguridad de la informaci√≥n..."

**Contextos recuperados (k=5):**
1. "ISO 27001 define requisitos para establecer, implementar..."
2. "Los controles de ISO 27001 incluyen gesti√≥n de acceso..."
3. "La norma ISO 27002 proporciona gu√≠as para implementar..."
4. "Documento sobre GDPR y privacidad de datos..."
5. "Manual de configuraci√≥n de firewalls..."

### C√°lculo de M√©tricas:

**Precision@5:**
- Contexto 1: Relevante ‚úÖ (overlap 45%)
- Contexto 2: Relevante ‚úÖ (overlap 38%)
- Contexto 3: Relevante ‚úÖ (overlap 25%)
- Contexto 4: No relevante ‚ùå (overlap 5%)
- Contexto 5: No relevante ‚ùå (overlap 2%)
- **Resultado: 3/5 = 0.60** (60%)

**Recall@5:**
- Respuesta usa informaci√≥n de contextos 1, 2, 3
- **Resultado: 3/5 = 0.60** (60%)

**Faithfulness:**
- Respuesta tiene 8 oraciones
- 7 oraciones respaldadas por contextos
- 1 oraci√≥n posiblemente inventada
- **Resultado: 7/8 = 0.875** (87.5%)

**Hallucination Rate:**
- **Resultado: 1 - 0.875 = 0.125** (12.5%)

**Answer Relevancy:**
- Pregunta tiene keywords: "ISO", "27001", "controles", "principales"
- Todas presentes en respuesta
- **Resultado: 4/4 = 1.0** (100%)

---

## ‚ú® Ventajas del Sistema Personalizado

1. ‚úÖ **Sin API Keys**: No requiere OpenAI/OpenRouter
2. ‚úÖ **Sin Costos**: Todo calculado localmente
3. ‚úÖ **R√°pido**: No hay latencia de llamadas a API externas
4. ‚úÖ **Confiable**: Sin problemas de compatibilidad de versiones
5. ‚úÖ **Transparente**: C√≥digo abierto, puedes ver c√≥mo se calcula cada m√©trica
6. ‚úÖ **Customizable**: Puedes ajustar thresholds y algoritmos
7. ‚úÖ **Offline**: Funciona sin internet

---

## üéì Para tu Tesis

### M√©tricas Clave a Reportar:

**Rendimiento:**
- Tiempo de respuesta promedio: `X.XX segundos`
- Time to First Token: `X.XX segundos`
- Throughput: `XX consultas/minuto`

**Precisi√≥n:**
- Precision@5: `XX%` - Documentos recuperados relevantes
- Recall@5: `XX%` - Cobertura de informaci√≥n
- Faithfulness: `XX%` - Respuestas basadas en evidencia
- Hallucination Rate: `XX%` - Informaci√≥n inventada

**Calidad:**
- Answer Relevancy: `XX%` - Respuestas pertinentes
- WER: `X.XX` - Exactitud de generaci√≥n (cuando aplique)

### Gr√°ficas Sugeridas:
1. Line chart: Evoluci√≥n de latencia en el tiempo
2. Bar chart: Comparaci√≥n de precision/recall/faithfulness
3. Scatter plot: Relaci√≥n entre complejidad de consulta y tiempo
4. Heatmap: M√©tricas por categor√≠a de documento

---

## üöÄ Pr√≥ximos Pasos

1. **Hacer consultas de prueba** para generar datos de m√©tricas
2. **Verificar el panel de administraci√≥n** para ver m√©tricas en tiempo real
3. **Exportar datos** para an√°lisis en tu tesis
4. **Ajustar thresholds** si es necesario (ej: cambiar 20% a 30% en precision)

---

## üìû Soporte

- Documentaci√≥n: Este archivo
- C√≥digo: `backend/api/custom_evaluator.py`
- Tests: Haz consultas y verifica en `/api/admin/metrics/`
