"""
Evaluador RAGAS para M√©tricas de Precisi√≥n
==========================================

Eval√∫a la calidad de las respuestas del sistema RAG usando el framework RAGAS.

M√©tricas implementadas:
- Faithfulness: ¬øLa respuesta est√° respaldada por el contexto?
- Answer Relevancy: ¬øLa respuesta es relevante a la pregunta?
- Context Precision: ¬øLos documentos recuperados son relevantes?
- Context Recall: ¬øSe recuperaron todos los documentos necesarios?
"""

import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

# Importar RAGAS solo cuando est√© disponible
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    )
    from datasets import Dataset
    RAGAS_AVAILABLE = True
    logger.info("‚úÖ RAGAS framework disponible")
except ImportError as e:
    RAGAS_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è RAGAS no disponible: {e}")
    logger.warning("üì¶ Instala con: pip install ragas datasets")


class RAGASEvaluator:
    """
    Evaluador de calidad para sistemas RAG usando RAGAS
    """
    
    def __init__(self):
        """Inicializar evaluador"""
        self.available = RAGAS_AVAILABLE
        
        if self.available:
            # Configurar m√©tricas a evaluar
            self.metrics = [
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall
            ]
            logger.info("üìä RAGASEvaluator inicializado con 4 m√©tricas")
        else:
            logger.warning("‚ö†Ô∏è RAGASEvaluator en modo simulado (RAGAS no instalado)")
    
    def evaluate_single(
        self,
        question: str,
        answer: str,
        contexts: List[str],
        ground_truth: Optional[str] = None
    ) -> Dict[str, float]:
        """
        Evaluar una sola consulta
        
        Args:
            question: La pregunta del usuario
            answer: La respuesta generada por el sistema
            contexts: Lista de contextos/documentos recuperados
            ground_truth: Respuesta correcta (opcional, para context_recall)
        
        Returns:
            Dict con las m√©tricas calculadas
        """
        if not self.available:
            return self._fallback_evaluation()
        
        try:
            # Preparar datos en formato RAGAS
            data = {
                "question": [question],
                "answer": [answer],
                "contexts": [contexts],
            }
            
            # Agregar ground_truth si est√° disponible
            if ground_truth:
                data["ground_truth"] = [ground_truth]
            
            # Crear dataset
            dataset = Dataset.from_dict(data)
            
            # Evaluar con m√©tricas disponibles
            metrics_to_use = self.metrics.copy()
            if not ground_truth:
                # Context recall y context precision requieren ground_truth (reference)
                if context_recall in metrics_to_use:
                    metrics_to_use.remove(context_recall)
                if context_precision in metrics_to_use:
                    metrics_to_use.remove(context_precision)
                logger.debug("‚ö†Ô∏è M√©tricas deshabilitadas (sin ground_truth): context_precision, context_recall")
            
            # Ejecutar evaluaci√≥n
            logger.debug(f"üîç Evaluando con RAGAS: {len(metrics_to_use)} m√©tricas")
            
            # Verificar que haya API key de OpenAI disponible
            import os
            if not os.getenv('OPENAI_API_KEY'):
                logger.warning("‚ö†Ô∏è OPENAI_API_KEY no configurada, usando evaluaci√≥n simulada")
                return self._fallback_evaluation()
            
            results = evaluate(dataset, metrics=metrics_to_use)
            
            # Extraer resultados
            scores = {
                'faithfulness': results.get('faithfulness', 0.0),
                'answer_relevancy': results.get('answer_relevancy', 0.0),
                'context_precision': results.get('context_precision', 0.0),
                'context_recall': results.get('context_recall', 0.0) if ground_truth else None
            }
            
            logger.info(f"‚úÖ Evaluaci√≥n RAGAS completada: {scores}")
            return scores
            
        except Exception as e:
            logger.error(f"‚ùå Error en evaluaci√≥n RAGAS: {e}")
            logger.info("üí° Tip: Configura OPENAI_API_KEY para usar RAGAS, o deshabilita RAGAS en settings")
            return self._fallback_evaluation()
    
    def evaluate_batch(
        self,
        questions: List[str],
        answers: List[str],
        contexts_list: List[List[str]],
        ground_truths: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Evaluar m√∫ltiples consultas en batch
        
        Args:
            questions: Lista de preguntas
            answers: Lista de respuestas generadas
            contexts_list: Lista de listas de contextos
            ground_truths: Lista de respuestas correctas (opcional)
        
        Returns:
            Dict con m√©tricas agregadas y por consulta
        """
        if not self.available:
            return {
                'aggregate': self._fallback_evaluation(),
                'per_query': [self._fallback_evaluation() for _ in questions]
            }
        
        try:
            # Preparar datos
            data = {
                "question": questions,
                "answer": answers,
                "contexts": contexts_list,
            }
            
            if ground_truths:
                data["ground_truth"] = ground_truths
            
            # Crear dataset
            dataset = Dataset.from_dict(data)
            
            # Determinar m√©tricas disponibles
            metrics_to_use = self.metrics.copy()
            if not ground_truths:
                # Context recall y context precision requieren ground_truth
                if context_recall in metrics_to_use:
                    metrics_to_use.remove(context_recall)
                if context_precision in metrics_to_use:
                    metrics_to_use.remove(context_precision)
            
            # Verificar que haya API key de OpenAI disponible
            import os
            if not os.getenv('OPENAI_API_KEY'):
                logger.warning("‚ö†Ô∏è OPENAI_API_KEY no configurada, usando evaluaci√≥n simulada para batch")
                return {
                    'aggregate': self._fallback_evaluation(),
                    'per_query': [self._fallback_evaluation() for _ in questions]
                }
            
            # Evaluar
            logger.info(f"üîç Evaluando batch de {len(questions)} consultas con RAGAS")
            results = evaluate(dataset, metrics=metrics_to_use)
            
            # Extraer resultados agregados
            aggregate = {
                'faithfulness': results.get('faithfulness', 0.0),
                'answer_relevancy': results.get('answer_relevancy', 0.0),
                'context_precision': results.get('context_precision', 0.0),
                'context_recall': results.get('context_recall', 0.0) if ground_truths else None
            }
            
            logger.info(f"‚úÖ Evaluaci√≥n batch completada: {aggregate}")
            
            return {
                'aggregate': aggregate,
                'results': results
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error en evaluaci√≥n batch RAGAS: {e}")
            return {
                'aggregate': self._fallback_evaluation(),
                'per_query': [self._fallback_evaluation() for _ in questions]
            }
    
    def calculate_precision_at_k(
        self,
        retrieved_docs: List[str],
        relevant_docs: List[str],
        k: int = 5
    ) -> float:
        """
        Calcular Precision@k manualmente
        
        Precision@k = (# docs relevantes en top-k) / k
        
        Args:
            retrieved_docs: Documentos recuperados (ordenados por score)
            relevant_docs: Documentos que son realmente relevantes
            k: N√∫mero de documentos a considerar
        
        Returns:
            Precision@k score [0.0 - 1.0]
        """
        if not retrieved_docs or k <= 0:
            return 0.0
        
        # Tomar solo los top-k
        top_k = retrieved_docs[:k]
        
        # Contar cu√°ntos son relevantes
        relevant_count = sum(1 for doc in top_k if doc in relevant_docs)
        
        precision = relevant_count / k
        logger.debug(f"üìä Precision@{k}: {relevant_count}/{k} = {precision:.2f}")
        
        return precision
    
    def calculate_recall_at_k(
        self,
        retrieved_docs: List[str],
        relevant_docs: List[str],
        k: int = 5
    ) -> float:
        """
        Calcular Recall@k manualmente
        
        Recall@k = (# docs relevantes recuperados en top-k) / (# total docs relevantes)
        
        Args:
            retrieved_docs: Documentos recuperados (ordenados por score)
            relevant_docs: Documentos que son realmente relevantes
            k: N√∫mero de documentos a considerar
        
        Returns:
            Recall@k score [0.0 - 1.0]
        """
        if not relevant_docs:
            return 0.0
        
        # Tomar solo los top-k
        top_k = retrieved_docs[:k]
        
        # Contar cu√°ntos relevantes fueron recuperados
        relevant_count = sum(1 for doc in top_k if doc in relevant_docs)
        
        recall = relevant_count / len(relevant_docs)
        logger.debug(f"üìä Recall@{k}: {relevant_count}/{len(relevant_docs)} = {recall:.2f}")
        
        return recall
    
    def calculate_hallucination_rate(self, faithfulness_score: float) -> float:
        """
        Calcular tasa de alucinaci√≥n basado en faithfulness
        
        Hallucination Rate = 1.0 - Faithfulness
        
        Args:
            faithfulness_score: Score de faithfulness [0.0 - 1.0]
        
        Returns:
            Hallucination rate [0.0 - 1.0]
        """
        hallucination = max(0.0, min(1.0, 1.0 - faithfulness_score))
        return hallucination
    
    def _fallback_evaluation(self) -> Dict[str, float]:
        """
        Evaluaci√≥n simulada cuando RAGAS no est√° disponible
        
        Retorna valores razonables para desarrollo/testing
        """
        return {
            'faithfulness': 0.85,
            'answer_relevancy': 0.82,
            'context_precision': 0.78,
            'context_recall': 0.75
        }
    
    def is_available(self) -> bool:
        """Verificar si RAGAS est√° disponible"""
        return self.available


# Instancia global del evaluador
_evaluator = None

def get_ragas_evaluator() -> RAGASEvaluator:
    """
    Obtener instancia singleton del evaluador RAGAS
    
    Returns:
        RAGASEvaluator instance
    """
    global _evaluator
    if _evaluator is None:
        _evaluator = RAGASEvaluator()
    return _evaluator


def evaluate_query(
    question: str,
    answer: str,
    contexts: List[str],
    ground_truth: Optional[str] = None
) -> Dict[str, float]:
    """
    Funci√≥n helper para evaluar una consulta
    
    Args:
        question: La pregunta del usuario
        answer: La respuesta generada
        contexts: Contextos recuperados
        ground_truth: Respuesta correcta (opcional)
    
    Returns:
        Dict con m√©tricas RAGAS
    """
    evaluator = get_ragas_evaluator()
    return evaluator.evaluate_single(question, answer, contexts, ground_truth)
