"""
Evaluador RAGAS para M√©tricas de Precisi√≥n
==========================================

Eval√∫a la calidad de las respuestas del sistema RAG usando el framework RAGAS.

M√©tricas implementadas:
- Faithfulness: ¬øLa respuesta est√° respaldada por el contexto?
- Answer Relevancy: ¬øLa respuesta es relevante a la pregunta?
- Context Precision: ¬øLos documentos recuperados son relevantes?
- Context Recall: ¬øSe recuperaron todos los documentos necesarios?
"""

import logging
import os
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

# Importar RAGAS solo cuando est√© disponible
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    )
    from datasets import Dataset
    from langchain_openai import ChatOpenAI
    RAGAS_AVAILABLE = True
    logger.info("‚úÖ RAGAS framework disponible")
except ImportError as e:
    RAGAS_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è RAGAS no disponible: {e}")
    logger.warning("üì¶ Instala con: pip install ragas datasets")


class RAGASEvaluator:
    """
    Evaluador de calidad para sistemas RAG usando RAGAS
    
    NOTA: RAGAS deshabilitado por defecto - usa m√©tricas simuladas.
    RAGAS requiere API keys de OpenAI y puede tener problemas de compatibilidad.
    Para an√°lisis de calidad, usa QueryMetrics que ya est√° funcionando.
    """
    
    def __init__(self):
        """Inicializar evaluador en modo simulado"""
        # RAGAS DESHABILITADO - siempre usar modo simulado
        self.available = False
        self.llm = None
        logger.info("üìä RAGASEvaluator inicializado en modo simulado (RAGAS deshabilitado)")
        logger.info("üí° Usando m√©tricas heur√≠sticas en lugar de RAGAS para mejor compatibilidad")
    
    def _setup_llm(self):
        """Configurar LLM para RAGAS usando OpenRouter o OpenAI"""
        try:
            # Prioridad: OPENROUTER_API_KEY > OPENAI_API_KEY
            openrouter_key = os.getenv('OPENROUTER_API_KEY')
            openai_key = os.getenv('OPENAI_API_KEY')
            
            if openrouter_key:
                # Usar OpenRouter (recomendado)
                # IMPORTANTE: Establecer como OPENAI_API_KEY para que RAGAS lo detecte
                os.environ['OPENAI_API_KEY'] = openrouter_key
                
                self.llm = ChatOpenAI(
                    model="google/gemma-2-9b-it",  # Modelo r√°pido y econ√≥mico
                    openai_api_key=openrouter_key,  # Par√°metro correcto
                    openai_api_base="https://openrouter.ai/api/v1",  # Base URL para OpenRouter
                    temperature=0.0,  # Determin√≠stico para evaluaciones
                    model_kwargs={"headers": {"HTTP-Referer": "http://localhost:8000"}}  # Requerido por OpenRouter
                )
                logger.info("‚úÖ RAGAS configurado con OpenRouter (gemma-2-9b-it)")
            elif openai_key:
                # Fallback a OpenAI
                self.llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    openai_api_key=openai_key,
                    temperature=0.0,
                )
                logger.info("‚úÖ RAGAS configurado con OpenAI (gpt-3.5-turbo)")
            else:
                logger.warning("‚ö†Ô∏è No se encontr√≥ OPENROUTER_API_KEY ni OPENAI_API_KEY")
                logger.warning("üí° Configura OPENROUTER_API_KEY para usar RAGAS")
                self.llm = None
        except Exception as e:
            logger.error(f"‚ùå Error configurando LLM para RAGAS: {e}")
            logger.info(f"üìã Detalles: {str(e)}")
            self.llm = None
    
    def evaluate_single(
        self,
        question: str,
        answer: str,
        contexts: List[str],
        ground_truth: Optional[str] = None
    ) -> Dict[str, float]:
        """
        Evaluar una sola consulta
        
        Args:
            question: La pregunta del usuario
            answer: La respuesta generada por el sistema
            contexts: Lista de contextos/documentos recuperados
            ground_truth: Respuesta correcta (opcional, para context_recall)
        
        Returns:
            Dict con las m√©tricas calculadas
        """
        if not self.available:
            return self._fallback_evaluation()
        
        try:
            # Preparar datos en formato RAGAS
            data = {
                "question": [question],
                "answer": [answer],
                "contexts": [contexts],
            }
            
            # Agregar ground_truth si est√° disponible
            if ground_truth:
                data["ground_truth"] = [ground_truth]
            
            # Crear dataset
            dataset = Dataset.from_dict(data)
            
            # Evaluar con m√©tricas disponibles
            metrics_to_use = self.metrics.copy()
            if not ground_truth:
                # Context recall y context precision requieren ground_truth (reference)
                if context_recall in metrics_to_use:
                    metrics_to_use.remove(context_recall)
                if context_precision in metrics_to_use:
                    metrics_to_use.remove(context_precision)
                logger.debug("‚ö†Ô∏è M√©tricas deshabilitadas (sin ground_truth): context_precision, context_recall")
            
            # Ejecutar evaluaci√≥n
            logger.debug(f"üîç Evaluando con RAGAS: {len(metrics_to_use)} m√©tricas")
            
            # Verificar que haya LLM configurado
            if not self.llm:
                logger.warning("‚ö†Ô∏è No hay LLM configurado para RAGAS, usando evaluaci√≥n simulada")
                return self._fallback_evaluation()
            
            # Ejecutar evaluaci√≥n con el LLM configurado
            results = evaluate(
                dataset, 
                metrics=metrics_to_use,
                llm=self.llm
            )
            
            # Extraer resultados - results es un DataFrame en versiones nuevas de RAGAS
            # Convertir a dict si es necesario
            if hasattr(results, 'to_dict'):
                results_dict = results.to_dict('list')
                scores = {
                    'faithfulness': float(results_dict.get('faithfulness', [0.0])[0]) if 'faithfulness' in results_dict else 0.0,
                    'answer_relevancy': float(results_dict.get('answer_relevancy', [0.0])[0]) if 'answer_relevancy' in results_dict else 0.0,
                    'context_precision': float(results_dict.get('context_precision', [0.0])[0]) if 'context_precision' in results_dict and ground_truth else 0.0,
                    'context_recall': float(results_dict.get('context_recall', [0.0])[0]) if 'context_recall' in results_dict and ground_truth else None
                }
            else:
                # Fallback para versiones antiguas
                scores = {
                    'faithfulness': getattr(results, 'faithfulness', 0.0),
                    'answer_relevancy': getattr(results, 'answer_relevancy', 0.0),
                    'context_precision': getattr(results, 'context_precision', 0.0) if ground_truth else 0.0,
                    'context_recall': getattr(results, 'context_recall', None) if ground_truth else None
                }
            
            logger.info(f"‚úÖ Evaluaci√≥n RAGAS completada: {scores}")
            return scores
            
        except Exception as e:
            logger.error(f"‚ùå Error en evaluaci√≥n RAGAS: {e}")
            logger.info("üí° Tip: Configura OPENROUTER_API_KEY o OPENAI_API_KEY para usar RAGAS")
            return self._fallback_evaluation()
    
    def evaluate_batch(
        self,
        questions: List[str],
        answers: List[str],
        contexts_list: List[List[str]],
        ground_truths: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Evaluar m√∫ltiples consultas en batch
        
        Args:
            questions: Lista de preguntas
            answers: Lista de respuestas generadas
            contexts_list: Lista de listas de contextos
            ground_truths: Lista de respuestas correctas (opcional)
        
        Returns:
            Dict con m√©tricas agregadas y por consulta
        """
        if not self.available:
            return {
                'aggregate': self._fallback_evaluation(),
                'per_query': [self._fallback_evaluation() for _ in questions]
            }
        
        try:
            # Preparar datos
            data = {
                "question": questions,
                "answer": answers,
                "contexts": contexts_list,
            }
            
            if ground_truths:
                data["ground_truth"] = ground_truths
            
            # Crear dataset
            dataset = Dataset.from_dict(data)
            
            # Determinar m√©tricas disponibles
            metrics_to_use = self.metrics.copy()
            if not ground_truths:
                # Context recall y context precision requieren ground_truth
                if context_recall in metrics_to_use:
                    metrics_to_use.remove(context_recall)
                if context_precision in metrics_to_use:
                    metrics_to_use.remove(context_precision)
            
            # Verificar que haya LLM configurado
            if not self.llm:
                logger.warning("‚ö†Ô∏è No hay LLM configurado para RAGAS, usando evaluaci√≥n simulada para batch")
                return {
                    'aggregate': self._fallback_evaluation(),
                    'per_query': [self._fallback_evaluation() for _ in questions]
                }
            
            # Evaluar
            logger.info(f"üîç Evaluando batch de {len(questions)} consultas con RAGAS")
            results = evaluate(
                dataset, 
                metrics=metrics_to_use,
                llm=self.llm
            )
            
            # Extraer resultados agregados - results es un DataFrame en versiones nuevas de RAGAS
            if hasattr(results, 'to_dict'):
                results_dict = results.to_dict('list')
                aggregate = {
                    'faithfulness': float(sum(results_dict.get('faithfulness', [0.0])) / len(questions)) if 'faithfulness' in results_dict else 0.0,
                    'answer_relevancy': float(sum(results_dict.get('answer_relevancy', [0.0])) / len(questions)) if 'answer_relevancy' in results_dict else 0.0,
                    'context_precision': float(sum(results_dict.get('context_precision', [0.0])) / len(questions)) if 'context_precision' in results_dict and ground_truths else 0.0,
                    'context_recall': float(sum(results_dict.get('context_recall', [0.0])) / len(questions)) if 'context_recall' in results_dict and ground_truths else None
                }
            else:
                # Fallback para versiones antiguas
                aggregate = {
                    'faithfulness': getattr(results, 'faithfulness', 0.0),
                    'answer_relevancy': getattr(results, 'answer_relevancy', 0.0),
                    'context_precision': getattr(results, 'context_precision', 0.0),
                    'context_recall': getattr(results, 'context_recall', 0.0) if ground_truths else None
                }
            
            logger.info(f"‚úÖ Evaluaci√≥n batch completada: {aggregate}")
            
            return {
                'aggregate': aggregate,
                'results': results
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error en evaluaci√≥n batch RAGAS: {e}")
            return {
                'aggregate': self._fallback_evaluation(),
                'per_query': [self._fallback_evaluation() for _ in questions]
            }
    
    def calculate_precision_at_k(
        self,
        retrieved_docs: List[str],
        relevant_docs: List[str],
        k: int = 5
    ) -> float:
        """
        Calcular Precision@k manualmente
        
        Precision@k = (# docs relevantes en top-k) / k
        
        Args:
            retrieved_docs: Documentos recuperados (ordenados por score)
            relevant_docs: Documentos que son realmente relevantes
            k: N√∫mero de documentos a considerar
        
        Returns:
            Precision@k score [0.0 - 1.0]
        """
        if not retrieved_docs or k <= 0:
            return 0.0
        
        # Tomar solo los top-k
        top_k = retrieved_docs[:k]
        
        # Contar cu√°ntos son relevantes
        relevant_count = sum(1 for doc in top_k if doc in relevant_docs)
        
        precision = relevant_count / k
        logger.debug(f"üìä Precision@{k}: {relevant_count}/{k} = {precision:.2f}")
        
        return precision
    
    def calculate_recall_at_k(
        self,
        retrieved_docs: List[str],
        relevant_docs: List[str],
        k: int = 5
    ) -> float:
        """
        Calcular Recall@k manualmente
        
        Recall@k = (# docs relevantes recuperados en top-k) / (# total docs relevantes)
        
        Args:
            retrieved_docs: Documentos recuperados (ordenados por score)
            relevant_docs: Documentos que son realmente relevantes
            k: N√∫mero de documentos a considerar
        
        Returns:
            Recall@k score [0.0 - 1.0]
        """
        if not relevant_docs:
            return 0.0
        
        # Tomar solo los top-k
        top_k = retrieved_docs[:k]
        
        # Contar cu√°ntos relevantes fueron recuperados
        relevant_count = sum(1 for doc in top_k if doc in relevant_docs)
        
        recall = relevant_count / len(relevant_docs)
        logger.debug(f"üìä Recall@{k}: {relevant_count}/{len(relevant_docs)} = {recall:.2f}")
        
        return recall
    
    def calculate_hallucination_rate(self, faithfulness_score: float) -> float:
        """
        Calcular tasa de alucinaci√≥n basado en faithfulness
        
        Hallucination Rate = 1.0 - Faithfulness
        
        Args:
            faithfulness_score: Score de faithfulness [0.0 - 1.0]
        
        Returns:
            Hallucination rate [0.0 - 1.0]
        """
        hallucination = max(0.0, min(1.0, 1.0 - faithfulness_score))
        return hallucination
    
    def _fallback_evaluation(self) -> Dict[str, float]:
        """
        Evaluaci√≥n simulada cuando RAGAS no est√° disponible
        
        Retorna valores razonables para desarrollo/testing
        """
        return {
            'faithfulness': 0.85,
            'answer_relevancy': 0.82,
            'context_precision': 0.78,
            'context_recall': 0.75
        }
    
    def is_available(self) -> bool:
        """Verificar si RAGAS est√° disponible"""
        return self.available


# Instancia global del evaluador
_evaluator = None

def get_ragas_evaluator() -> RAGASEvaluator:
    """
    Obtener instancia singleton del evaluador RAGAS
    
    Returns:
        RAGASEvaluator instance
    """
    global _evaluator
    if _evaluator is None:
        _evaluator = RAGASEvaluator()
    return _evaluator


def evaluate_query(
    question: str,
    answer: str,
    contexts: List[str],
    ground_truth: Optional[str] = None
) -> Dict[str, float]:
    """
    Funci√≥n helper para evaluar una consulta
    
    Args:
        question: La pregunta del usuario
        answer: La respuesta generada
        contexts: Contextos recuperados
        ground_truth: Respuesta correcta (opcional)
    
    Returns:
        Dict con m√©tricas RAGAS
    """
    evaluator = get_ragas_evaluator()
    return evaluator.evaluate_single(question, answer, contexts, ground_truth)
